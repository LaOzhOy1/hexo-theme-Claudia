{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/Claudia/source/js/highlight.pack.js","path":"js/highlight.pack.js","modified":0,"renderable":1},{"_id":"themes/Claudia/source/js/img_zoom.js","path":"js/img_zoom.js","modified":0,"renderable":1},{"_id":"themes/Claudia/source/js/jquery-3.6.1.min.js","path":"js/jquery-3.6.1.min.js","modified":0,"renderable":1},{"_id":"themes/Claudia/source/js/common.js","path":"js/common.js","modified":0,"renderable":1},{"_id":"themes/Claudia/source/js/jquery-fancybox.min.js","path":"js/jquery-fancybox.min.js","modified":0,"renderable":1},{"_id":"themes/Claudia/source/js/post.js","path":"js/post.js","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/about.scss","path":"style/about.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/archive.scss","path":"style/archive.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/base.scss","path":"style/base.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/widget-header.scss","path":"style/widget-header.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/post.scss","path":"style/post.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/widget-post-list.scss","path":"style/widget-post-list.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/images/aha.png","path":"images/aha.png","modified":0,"renderable":1},{"_id":"themes/Claudia/source/images/favicon.ico","path":"images/favicon.ico","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/common/bulma.css","path":"style/common/bulma.css","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/common/helper.scss","path":"style/common/helper.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/common/jquery.fancybox.min.css","path":"style/common/jquery.fancybox.min.css","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/common/variable.scss","path":"style/common/variable.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/themes/default-dark.scss","path":"style/themes/default-dark.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/themes/default-light.scss","path":"style/themes/default-light.scss","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/themes/highlight-theme-light.css","path":"style/themes/highlight-theme-light.css","modified":0,"renderable":1},{"_id":"themes/Claudia/source/style/themes/theme.scss","path":"style/themes/theme.scss","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/DeepSeek发展历程.md","hash":"f424745c4518a1fd163d3d7bcfc1cde145d976e2","modified":1740866377185},{"_id":"source/_posts/DeepSeek发展历程/p1_header.png","hash":"6ed51782b950e15eadaccdadbe55d4acdefbbbb2","modified":1739378201018},{"_id":"source/_posts/DeepSeek发展历程/p1_money_cost.png","hash":"59c49b83e9952a663f26d7fecf95f2a6cb0bc84e","modified":1740680455813},{"_id":"source/_posts/DeepSeek发展历程/p1_r1_compare_other.png","hash":"7dc5fc5254547c5e8cbbc9caebb7614e2a337079","modified":1739366271765},{"_id":"themes/Claudia/CHANGELOG.md","hash":"59b1995c016ad26343409f7ddff9f5feb75038e2","modified":1735959738295},{"_id":"themes/Claudia/.gitignore","hash":"bd20d54c57507594cd16a21021c3600f9311a1f5","modified":1735959738295},{"_id":"themes/Claudia/CODE_OF_CONDUCT.md","hash":"787b987cd6079f93c7846b69c3b4dfa41cb3ac03","modified":1735959738295},{"_id":"themes/Claudia/LICENSE","hash":"9812afb9d0aa8596067b6fd30cf6089345b7b678","modified":1735959738295},{"_id":"themes/Claudia/CONTRIBUTING.md","hash":"618215987cc9a774c37cc70efa1cb8545457a49c","modified":1735959738295},{"_id":"themes/Claudia/package.json","hash":"c59cb2089c51ed56deb2df69171d7adc63c1148f","modified":1735959738297},{"_id":"themes/Claudia/layout/about.pug","hash":"98dea176f76053d5deaf35ed25518d218d70be7d","modified":1735959738296},{"_id":"themes/Claudia/README.md","hash":"640eb3c39fff76193edd50cd60334e04c78a4ef1","modified":1739278772184},{"_id":"themes/Claudia/README-CN.md","hash":"4688e3f1046585e3d7ae79f63104ed301864a240","modified":1735959738296},{"_id":"themes/Claudia/layout/archive.pug","hash":"68c709495bc39a659d9c4b19216714a5ac2b5579","modified":1735959738296},{"_id":"themes/Claudia/layout/category.pug","hash":"2147f3d66640bc6604c9b15325a480d196a4df3d","modified":1735959738296},{"_id":"themes/Claudia/layout/index.pug","hash":"96b3b857b19b62823f84a164d20293cc16d891aa","modified":1735959738296},{"_id":"themes/Claudia/layout/page.pug","hash":"6c5db904a03adb4794b7ada222389da12d395bc7","modified":1735959738296},{"_id":"themes/Claudia/_config.yml","hash":"8af6091425351b9802e63437a60076122147e9b1","modified":1735980474776},{"_id":"themes/Claudia/languages/en.yml","hash":"66d680dcaaa2374c8a400c7266e48a4f662a9035","modified":1735959738296},{"_id":"themes/Claudia/languages/zh-CN.yml","hash":"dfaa6ca86d6dc041616f09e5e0b16221bdf7e122","modified":1735959738296},{"_id":"themes/Claudia/.github/FUNDING.yml","hash":"dd672081ec4678929f6c1ac3ebbef4d990291ecd","modified":1735959738295},{"_id":"themes/Claudia/.github/PULL_REQUEST_TEMPLATE.md","hash":"35751990a36fffe5a5f6fd682452fe5594cadde9","modified":1735959738295},{"_id":"themes/Claudia/layout/tag.pug","hash":"16dac6e0a6ef939ceb6adb21dfbe0276538ff269","modified":1735959738296},{"_id":"themes/Claudia/layout/post.pug","hash":"ebbb41aa73d3c702b983cfe740261b5b366d1145","modified":1735959738296},{"_id":"themes/Claudia/layout/widget/base.pug","hash":"21a5eab68859adedc9997581fc54e848693c17d4","modified":1735959738296},{"_id":"themes/Claudia/layout/widget/methods.pug","hash":"fa62f6ad95d1a4cda5038595d19d4d11b4b39e17","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-archives.pug","hash":"e6bcf8b09a5e06119baf3f97f7798ef7600ab65c","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-header.pug","hash":"5e9b2035b31372c2bb11db6efe0e36b7df64d884","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-categories.pug","hash":"ea3bcd5f5fb06e26d8b67f30f224e7c129672980","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-profile.pug","hash":"fbe18f3dd5e8d2850f8e6510da4fbd499d5096f0","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-post-list.pug","hash":"f034a4b8231f7933d06e364671b11847c74b9a5c","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-search.pug","hash":"c17612dd4ae2d439d757818ec0e4215d71dd9ad5","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-recent.pug","hash":"7512dceae690ea219d562a6e450d633e29916072","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-tag.pug","hash":"714a05148758e1e3cc12635c875cb6ef1753c6ab","modified":1735959738297},{"_id":"themes/Claudia/layout/widget/widget-sns.pug","hash":"8ec154d321b36a5f3bf5974c75668b45ec660c1d","modified":1735959738297},{"_id":"themes/Claudia/source/js/common.js","hash":"7f62c8c148ee0b528a2bea47b248b2a7f5501142","modified":1735959738302},{"_id":"themes/Claudia/source/js/img_zoom.js","hash":"a384c3a60fcbdad813cb8340200dfb07ebbcc48c","modified":1735959738302},{"_id":"themes/Claudia/source/js/post.js","hash":"04edd583f103ba444d8174d55e7f45e27b8f2549","modified":1735959738303},{"_id":"themes/Claudia/.github/ISSUE_TEMPLATE/bug_report.md","hash":"b38365fec9b6cac6bbb75441082f041c4efd35bf","modified":1735959738295},{"_id":"themes/Claudia/source/style/about.scss","hash":"4a1beebb317c598b11fc4815e70b07bbb6d2aed7","modified":1735959738303},{"_id":"themes/Claudia/source/style/archive.scss","hash":"cd1dc16dfa7c482cc88ddabaf8c9a459299a98cf","modified":1735959738303},{"_id":"themes/Claudia/source/style/base.scss","hash":"da7457c952b6b433f273d7ecc562d24dcd4aa431","modified":1735959738303},{"_id":"themes/Claudia/source/style/widget-header.scss","hash":"18782c2ab206abbeb0ee59632864251acfd20d85","modified":1735959738304},{"_id":"themes/Claudia/.github/ISSUE_TEMPLATE/feature_request.md","hash":"9d8447814c3ac93d7fbd336015e7ef80c4a32831","modified":1735959738295},{"_id":"themes/Claudia/source/style/post.scss","hash":"8a3520a0ceb22a2fde35b09485e2df6dd5a23b6f","modified":1735959738304},{"_id":"themes/Claudia/source/style/widget-post-list.scss","hash":"53ea5dc4fb868f3553a5c117e528082ba3c7961c","modified":1735959738304},{"_id":"themes/Claudia/source/style/common/helper.scss","hash":"7cd0982fe839e760523f8f85ea6f06f2b5d2111a","modified":1735959738303},{"_id":"themes/Claudia/source/style/common/variable.scss","hash":"fffe6eacfa4d814626a1e1d84dc651233eded060","modified":1735959738304},{"_id":"themes/Claudia/source/images/favicon.ico","hash":"96b9a549337c2bec483c2879eeafa4d1f8748fed","modified":1735959738302},{"_id":"themes/Claudia/source/style/common/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1735959738304},{"_id":"themes/Claudia/source/style/themes/default-dark.scss","hash":"0cbdc5738503f55e5b84e1bd00e445c98c7d56d2","modified":1735959738304},{"_id":"themes/Claudia/source/style/themes/default-light.scss","hash":"23e7c1f87e252db80d34d36a2129e98dde7b1b55","modified":1735959738304},{"_id":"themes/Claudia/source/style/themes/highlight-theme-light.css","hash":"f7b19080f00e10723bc86e6819fc25143a0137c5","modified":1735959738304},{"_id":"themes/Claudia/source/style/themes/theme.scss","hash":"caf7517c9200bbf7152a7f5edba4298fbf9ae629","modified":1735959738304},{"_id":"themes/Claudia/img.png","hash":"2239d04aa7d31850b024d3ccac609fbf9dcb98d2","modified":1735959738296},{"_id":"themes/Claudia/source/images/aha.png","hash":"78ccc1d1d15f0a8ba5accb0fa23c554139ea8cec","modified":1735960419428},{"_id":"themes/Claudia/source/js/jquery-fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1735959738303},{"_id":"themes/Claudia/source/js/jquery-3.6.1.min.js","hash":"0fa72756e48c33a6feeace1ffa5d790d58b53729","modified":1735959738303},{"_id":"themes/Claudia/source/js/highlight.pack.js","hash":"2ef9bbbc688ce413686ce0eb64d0b25af4ea34e2","modified":1735959738302},{"_id":"themes/Claudia/screenshot/BuyMeCoffeeQRCode.png","hash":"df14bf998f23f9e8a323e2d79802e887875c8842","modified":1735959738297},{"_id":"themes/Claudia/source/style/common/bulma.css","hash":"7ede761951c6f274850a1038416559a8f648c493","modified":1735959738303},{"_id":"themes/Claudia/screenshot/claudia-cover.png","hash":"757680cd3648e3569d6a18abaf33180dc427d620","modified":1735959738302},{"_id":"themes/Claudia/screenshot/claudia-cover-v2.png","hash":"f477d90d327a638a46b3caa172332e931955a532","modified":1735959738300},{"_id":"public/atom.xml","hash":"f10d63d09a9e9ae4560b27796d9cb314e7e24fee","modified":1740866380040},{"_id":"public/search.xml","hash":"1bc84a355242e909057f909d8215a118ae5a8963","modified":1740866380040},{"_id":"public/2025/02/11/DeepSeek发展历程/index.html","hash":"405eb24641233a2bac6b76828f522f6be93c1b20","modified":1740866380040},{"_id":"public/archives/index.html","hash":"ec841534085d7810e770a4e05e9940fc857ce115","modified":1740866380040},{"_id":"public/archives/2025/index.html","hash":"4f18a852eb3f3536d474210b378add47eeca261b","modified":1740866380040},{"_id":"public/archives/2025/02/index.html","hash":"4f18a852eb3f3536d474210b378add47eeca261b","modified":1740866380040},{"_id":"public/index.html","hash":"b585acb852d609fa0f7f589e25031af537833e6a","modified":1740866380040},{"_id":"public/images/aha.png","hash":"78ccc1d1d15f0a8ba5accb0fa23c554139ea8cec","modified":1740866380040},{"_id":"public/2025/02/11/DeepSeek发展历程/p1_header.png","hash":"6ed51782b950e15eadaccdadbe55d4acdefbbbb2","modified":1740866380040},{"_id":"public/images/favicon.ico","hash":"96b9a549337c2bec483c2879eeafa4d1f8748fed","modified":1740866380040},{"_id":"public/2025/02/11/DeepSeek发展历程/p1_r1_compare_other.png","hash":"7dc5fc5254547c5e8cbbc9caebb7614e2a337079","modified":1740866380040},{"_id":"public/2025/02/11/DeepSeek发展历程/p1_money_cost.png","hash":"59c49b83e9952a663f26d7fecf95f2a6cb0bc84e","modified":1740866380040},{"_id":"public/js/img_zoom.js","hash":"a384c3a60fcbdad813cb8340200dfb07ebbcc48c","modified":1740866380040},{"_id":"public/js/common.js","hash":"7f62c8c148ee0b528a2bea47b248b2a7f5501142","modified":1740866380040},{"_id":"public/js/jquery-3.6.1.min.js","hash":"0fa72756e48c33a6feeace1ffa5d790d58b53729","modified":1740866380040},{"_id":"public/js/jquery-fancybox.min.js","hash":"6181412e73966696d08e1e5b1243a572d0f22ba6","modified":1740866380040},{"_id":"public/js/post.js","hash":"04edd583f103ba444d8174d55e7f45e27b8f2549","modified":1740866380040},{"_id":"public/style/common/jquery.fancybox.min.css","hash":"1be9b79be02a1cfc5d96c4a5e0feb8f472babd95","modified":1740866380040},{"_id":"public/js/highlight.pack.js","hash":"2ef9bbbc688ce413686ce0eb64d0b25af4ea34e2","modified":1740866380040},{"_id":"public/style/themes/highlight-theme-light.css","hash":"f7b19080f00e10723bc86e6819fc25143a0137c5","modified":1740866380040},{"_id":"public/style/common/bulma.css","hash":"7ede761951c6f274850a1038416559a8f648c493","modified":1740866380040},{"_id":"public/style/themes/default-dark.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1740866380040},{"_id":"public/style/themes/default-light.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1740866380040},{"_id":"public/style/common/variable.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1740866380040},{"_id":"public/style/archive.css","hash":"fc79cddde5b0ee019889337bb3098d73bb6824cb","modified":1740866380040},{"_id":"public/style/about.css","hash":"54acbee918d1f3fb104e91ca074d397ccd38f5ed","modified":1740866380040},{"_id":"public/style/widget-header.css","hash":"80113d3a162a87b0574b478700242e8bb8cc9cf0","modified":1740866380040},{"_id":"public/style/common/helper.css","hash":"0b7612eebf65156aceec2f91b229ddeb38092456","modified":1740866380040},{"_id":"public/style/themes/theme.css","hash":"a334f9eaa157eda2bce485b22237a5d24cdf7d70","modified":1740866380040},{"_id":"public/style/post.css","hash":"9e42bd627735bf438d97c1709f76b853f2989a25","modified":1740866380040},{"_id":"public/style/base.css","hash":"e83bb81e864a736a80abe2901485cf43a1d86a87","modified":1740866380040},{"_id":"public/style/widget-post-list.css","hash":"f2b934eb3827d33353661f6480e2c320c0527bd4","modified":1740866380040}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"DeepSeek的发展","date":"2025-02-11T13:12:23.000Z","_content":"### 简介\n\nDeepSeek 依托其独特的架构，以极低丹成本卷赢了OpenAI等公司花费巨大成本\n打造的AI模型，各大依托大语言模型提供服务科技公司纷纷将自己的后端替换为更廉价的DeepSeek，\n它有什么样的魅力，它的出现又带来怎样翻天覆地的变化。\n\n![p1_header.png](DeepSeek%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/p1_header.png)\n\n### 昂贵的AI模型\n人类的大脑，内部有有许许多多的神经元，依靠大脑，人们可以对事物做出判断，付诸行动。\nAI模型就像一颗人工大脑，它通过大量的样本学习，上百亿的参数参与计算，将习得经验\n存储在硬盘中；然而，快速计算需要有超强算力的GPU、文件存储需要高IO，大缓存的存储后台 、\n海量的人工标记样本也是一笔不菲的费用。\n\n现今，各家大厂都推出了各自的AI模型，其中OpenAI公司的OpenAI o1在数学，代码和推理任务\n的效果最为优秀，然而，OpenAI技术是闭源的，并不对外公开，大众能看到的是其训练的所需巨额显卡算力。\n然而又有多少家公司能像OpenAI一样投入这般财力和人力搞模型呢？\n\n答案是很少，英伟达作为显卡行业的佼佼者， 其显卡在AI训练的支持上最为先进，在行业里已经形成垄断 \n，价格也是不断上涨。但这并不能阻止投资者的疯狂，一个公司要训练一个智能的模型，购入大量价格高昂的显卡，\n是不可避免的花销，对于普通人，若想要使用AI功能，只能通过网络API调用大厂模型开放的接口，在其基础上二次开发。\n\n![money_cost.png](DeepSeek发展历程/p1_money_cost.png)\n\n### 百花齐放\n过去几年，基于仅解码器 Transformer（Vaswani 等人，2017 年）的大型语言模型（LLM）\n已逐渐成为实现通用人工智能 (AGI) 的基石和途径。 这一波浪潮是由闭源产品引领，例如 ChatGPT（OpenAI，2022）、\nClaude（Anthropic，2023）和 Bard（Google，2023），这些产品利用了大量的计算资源和大量标注成本进行开发。\n其中，LLaMA 系列模型创建了一个高效稳定的架构，构建了从 7B 到 70B 参数的性能优异模型\n跟随 LLaMA，开源社区主要集中在训练固定大小（7B、13B、34B 和 70B）的高质量模型上，常常忽略对大型语言模型 (LLM) 规模规律的研究探索\n\n### DeepSeek V1 LLMs——2万亿个大型数据集训练的开源大语言模型\nDeepSeek的大语言模型，与现有的大语言模型在宏观设计方面略有不同。具体来说，DeepSeek LLM 7B 是一个 30 层网络，\n而 DeepSeek LLM 67B 有 95 层。这些层级调整，在保持与其他开源模型参数一致性的同时，\n也有利于模型流水线划分，优化训练和推理。而外的，他们采取分阶段微调的方式，第一阶段涉及使用所有可用数据进行微调，\n而第二阶段专门关注使用对话数据进行微调，解决小型模型需要在数学和代码数据集上进行更长时间的微调后，模型对话能力被损坏的问题\n\n### DeepSeek MOE —— 创新架构，经济高效\n\n在大型语言模型时代，DeepSeek团队引入龙混合专家（Mixture-of-Experts, MoE）架构，\n对比先前的大数据模型，是一种有前途的架构，它的特点如下：\n\n细粒度专家分割：\n在保持参数数量不变的同时，我们通过分割FFN中间隐藏维度将专家分割成更细粒度。\n相应地，在保持恒定计算成本的情况下，我们还激活了更细粒度的专家，以实现更灵活、\n适应性更强的激活专家组合。细粒度的专家细分允许将不同的知识更精细地分解，\n并更精确地学习到不同的专家中，每个专家都将保持更高的专业水平。\n此外，组合活跃专家的灵活性增加也有助于更准确、更有针对性地获取知识。\n\n共享专家隔离：\n我们隔离某些专家，作为始终处于活动状态的共享专家，旨在在不同的环境中捕获和巩固共同知识。\n通过将共同知识压缩到这些共享专家中，将减少其他路由专家之间的冗余。\n这可以提高参数效率，并确保每个路由专家通过专注于独特的方面来保持专业性。\nDeepSeekMoE中的这些架构创新为训练一个参数高效的MoE语言模型提供了机会，在这个模型中，\n每个专家都是高度专业化的。\n\nDeepSeekMoE扩展到16B总参数的更大规模，在2T token上训练DeepSeekMoE 16B，\n并展示了其与DeepSeek 7B和LLaMA2 7B相当的卓越性能，仅使用了约40%的计算量。\n将DeepSeek MoE扩展到145B参数。DeepSeekMoE 145B仍然比GShard架构具有实质性的优势，\n并且与DeepSeek 67B的性能相当，只使用了28.5%（甚至18.2%）的计算量。\n\n### DeepSeek V2 —— 强大、经济且高效的混合专家语言模型\nDeepSeek-V2，一个支持 128K 上下文长度的大型 MoE 语言模型。 除了强大的性能之外，\n它还具有经济训练和高效推理的特点，这得益于其包括 MLA 和 DeepSeekMoE 在内的创新架构。\n\nMLA：\n除此之外，DeepSeek-V2设计了一种创新的注意力机制，称为多头潜在注意力（MLA）。\n配备低秩键值联合压缩，MLA的性能优于MHA，但需要的KV缓存量要少得多。\n它利用低秩键值联合压缩来消除推理时间键值缓存的瓶颈，从而支持高效的推理。\n\nDeepSeek-V2 一个支持 128K 上下文长度的大型 MoE 语言模型，仍处于Transformer架构中（Vaswani等人，2017），\n只是其中每个Transformer 块由一个注意力模块和一个前馈网络（FFN）组成。\n\n与 DeepSeek 67B 相比，DeepSeek-V2 实现了显著增强的性能，同时节省了 42.5% 的训练成本、减少了 93.3% 的 KV 缓存、并将最大生成吞吐量提升至 5.76 倍。在由 8.1T 标记组成的高质量多源语料库上对 DeepSeek-V2 进行预训练，并进一步执行有监督微调 (SFT) 和强化学习 (RL) 以充分发挥其潜力。\n评估结果表明，即使只有 21B 激活参数，DeepSeek-V2 及其聊天版本仍然在开源模型中实现了顶级性能。\n\n### DeepSeek R1 强化学习激发潜能\n先前的Deepseek版本在很大程度上依赖大量的监督数据来提高模型性能。\n在DeepSeek团队研究中，各代产品展示的推理能力都可以通过大规模强化学习（RL）显著提升，\n即使不使用监督微调（SFT）作为冷启动。此外，性能还可以通过加入少量冷启动数据进一步增强。\n\n产品列表：\n（1）DeepSeek-R1-Zero，它直接将强化学习应用于基础模型，无需任何 SFT 数据；\n（2）DeepSeek-R1，它从经过数千条长链式推理（CoT）示例微调的检查点开始应用强化学习；\n（3）从 DeepSeek-R1 蒸馏推理能力到小型稠密模型。\n\n比较：\nDeepSeek-R1在FRAMES方面表现出色，FRAMES是一项依赖于上下文的长期QA任务，\n展示了其强大的文档分析能力。这突显了推理模型在人工智能驱动的搜索和数据分析任务中的潜力。\n在事实基准SimpleQA上，DeepSeek-R1的表现优于DeepSeek-V3，展示了其处理基于事实的查询的能力。\n在OpenAI-o1超过GPT-4o的情况下，也观察到了类似的趋势。\n然而，在中国SimpleQA基准测试中，DeepSeek-R1的表现不如DeepSeek-V3，\n主要是因为它倾向于在安全RL后拒绝回答某些查询。如果没有安全RL，DeepSeek-R1的准确率可以达到70%以\n\n\n### 未来DeepSeek方向\n通用能力：目前，DeepSeek-R1在函数调用、多轮对话、复杂角色扮演和JSON输出等任务上的能力仍不及DeepSeek-V3。未来，我们计划探索如何利用长链推理(CoT)来增强这些领域的任务能力。\n语言混合：DeepSeek-R1目前优化了中文和英文，因此在处理其他语言的查询时可能会出现语言混用的问题。例如，DeepSeek-R1可能会使用英文进行推理和回答，即使查询本身是其他语言。我们计划在未来的更新中解决这一局限性。\n提示工程：在评估DeepSeek-R1时，我们观察到它对提示非常敏感。少量示例提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零-shot设置指定输出格式，以获得最佳结果。\n软件工程任务：由于评估时间较长，影响了强化学习过程的效率，大规模强化学习尚未在软件工程任务中广泛应用。因此，DeepSeek-R1在软件工程基准测试上未能相较于DeepSeek-V3表现出显著提升。未来版本将通过在软件工程数据上实施拒绝采样或在强化学习过程中引入异步评估来提高效率。\n\n\n","source":"_posts/DeepSeek发展历程.md","raw":"---\ntitle: DeepSeek的发展\ndate: 2025-02-11 21:12:23\ntags:\n---\n### 简介\n\nDeepSeek 依托其独特的架构，以极低丹成本卷赢了OpenAI等公司花费巨大成本\n打造的AI模型，各大依托大语言模型提供服务科技公司纷纷将自己的后端替换为更廉价的DeepSeek，\n它有什么样的魅力，它的出现又带来怎样翻天覆地的变化。\n\n![p1_header.png](DeepSeek%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/p1_header.png)\n\n### 昂贵的AI模型\n人类的大脑，内部有有许许多多的神经元，依靠大脑，人们可以对事物做出判断，付诸行动。\nAI模型就像一颗人工大脑，它通过大量的样本学习，上百亿的参数参与计算，将习得经验\n存储在硬盘中；然而，快速计算需要有超强算力的GPU、文件存储需要高IO，大缓存的存储后台 、\n海量的人工标记样本也是一笔不菲的费用。\n\n现今，各家大厂都推出了各自的AI模型，其中OpenAI公司的OpenAI o1在数学，代码和推理任务\n的效果最为优秀，然而，OpenAI技术是闭源的，并不对外公开，大众能看到的是其训练的所需巨额显卡算力。\n然而又有多少家公司能像OpenAI一样投入这般财力和人力搞模型呢？\n\n答案是很少，英伟达作为显卡行业的佼佼者， 其显卡在AI训练的支持上最为先进，在行业里已经形成垄断 \n，价格也是不断上涨。但这并不能阻止投资者的疯狂，一个公司要训练一个智能的模型，购入大量价格高昂的显卡，\n是不可避免的花销，对于普通人，若想要使用AI功能，只能通过网络API调用大厂模型开放的接口，在其基础上二次开发。\n\n![money_cost.png](DeepSeek发展历程/p1_money_cost.png)\n\n### 百花齐放\n过去几年，基于仅解码器 Transformer（Vaswani 等人，2017 年）的大型语言模型（LLM）\n已逐渐成为实现通用人工智能 (AGI) 的基石和途径。 这一波浪潮是由闭源产品引领，例如 ChatGPT（OpenAI，2022）、\nClaude（Anthropic，2023）和 Bard（Google，2023），这些产品利用了大量的计算资源和大量标注成本进行开发。\n其中，LLaMA 系列模型创建了一个高效稳定的架构，构建了从 7B 到 70B 参数的性能优异模型\n跟随 LLaMA，开源社区主要集中在训练固定大小（7B、13B、34B 和 70B）的高质量模型上，常常忽略对大型语言模型 (LLM) 规模规律的研究探索\n\n### DeepSeek V1 LLMs——2万亿个大型数据集训练的开源大语言模型\nDeepSeek的大语言模型，与现有的大语言模型在宏观设计方面略有不同。具体来说，DeepSeek LLM 7B 是一个 30 层网络，\n而 DeepSeek LLM 67B 有 95 层。这些层级调整，在保持与其他开源模型参数一致性的同时，\n也有利于模型流水线划分，优化训练和推理。而外的，他们采取分阶段微调的方式，第一阶段涉及使用所有可用数据进行微调，\n而第二阶段专门关注使用对话数据进行微调，解决小型模型需要在数学和代码数据集上进行更长时间的微调后，模型对话能力被损坏的问题\n\n### DeepSeek MOE —— 创新架构，经济高效\n\n在大型语言模型时代，DeepSeek团队引入龙混合专家（Mixture-of-Experts, MoE）架构，\n对比先前的大数据模型，是一种有前途的架构，它的特点如下：\n\n细粒度专家分割：\n在保持参数数量不变的同时，我们通过分割FFN中间隐藏维度将专家分割成更细粒度。\n相应地，在保持恒定计算成本的情况下，我们还激活了更细粒度的专家，以实现更灵活、\n适应性更强的激活专家组合。细粒度的专家细分允许将不同的知识更精细地分解，\n并更精确地学习到不同的专家中，每个专家都将保持更高的专业水平。\n此外，组合活跃专家的灵活性增加也有助于更准确、更有针对性地获取知识。\n\n共享专家隔离：\n我们隔离某些专家，作为始终处于活动状态的共享专家，旨在在不同的环境中捕获和巩固共同知识。\n通过将共同知识压缩到这些共享专家中，将减少其他路由专家之间的冗余。\n这可以提高参数效率，并确保每个路由专家通过专注于独特的方面来保持专业性。\nDeepSeekMoE中的这些架构创新为训练一个参数高效的MoE语言模型提供了机会，在这个模型中，\n每个专家都是高度专业化的。\n\nDeepSeekMoE扩展到16B总参数的更大规模，在2T token上训练DeepSeekMoE 16B，\n并展示了其与DeepSeek 7B和LLaMA2 7B相当的卓越性能，仅使用了约40%的计算量。\n将DeepSeek MoE扩展到145B参数。DeepSeekMoE 145B仍然比GShard架构具有实质性的优势，\n并且与DeepSeek 67B的性能相当，只使用了28.5%（甚至18.2%）的计算量。\n\n### DeepSeek V2 —— 强大、经济且高效的混合专家语言模型\nDeepSeek-V2，一个支持 128K 上下文长度的大型 MoE 语言模型。 除了强大的性能之外，\n它还具有经济训练和高效推理的特点，这得益于其包括 MLA 和 DeepSeekMoE 在内的创新架构。\n\nMLA：\n除此之外，DeepSeek-V2设计了一种创新的注意力机制，称为多头潜在注意力（MLA）。\n配备低秩键值联合压缩，MLA的性能优于MHA，但需要的KV缓存量要少得多。\n它利用低秩键值联合压缩来消除推理时间键值缓存的瓶颈，从而支持高效的推理。\n\nDeepSeek-V2 一个支持 128K 上下文长度的大型 MoE 语言模型，仍处于Transformer架构中（Vaswani等人，2017），\n只是其中每个Transformer 块由一个注意力模块和一个前馈网络（FFN）组成。\n\n与 DeepSeek 67B 相比，DeepSeek-V2 实现了显著增强的性能，同时节省了 42.5% 的训练成本、减少了 93.3% 的 KV 缓存、并将最大生成吞吐量提升至 5.76 倍。在由 8.1T 标记组成的高质量多源语料库上对 DeepSeek-V2 进行预训练，并进一步执行有监督微调 (SFT) 和强化学习 (RL) 以充分发挥其潜力。\n评估结果表明，即使只有 21B 激活参数，DeepSeek-V2 及其聊天版本仍然在开源模型中实现了顶级性能。\n\n### DeepSeek R1 强化学习激发潜能\n先前的Deepseek版本在很大程度上依赖大量的监督数据来提高模型性能。\n在DeepSeek团队研究中，各代产品展示的推理能力都可以通过大规模强化学习（RL）显著提升，\n即使不使用监督微调（SFT）作为冷启动。此外，性能还可以通过加入少量冷启动数据进一步增强。\n\n产品列表：\n（1）DeepSeek-R1-Zero，它直接将强化学习应用于基础模型，无需任何 SFT 数据；\n（2）DeepSeek-R1，它从经过数千条长链式推理（CoT）示例微调的检查点开始应用强化学习；\n（3）从 DeepSeek-R1 蒸馏推理能力到小型稠密模型。\n\n比较：\nDeepSeek-R1在FRAMES方面表现出色，FRAMES是一项依赖于上下文的长期QA任务，\n展示了其强大的文档分析能力。这突显了推理模型在人工智能驱动的搜索和数据分析任务中的潜力。\n在事实基准SimpleQA上，DeepSeek-R1的表现优于DeepSeek-V3，展示了其处理基于事实的查询的能力。\n在OpenAI-o1超过GPT-4o的情况下，也观察到了类似的趋势。\n然而，在中国SimpleQA基准测试中，DeepSeek-R1的表现不如DeepSeek-V3，\n主要是因为它倾向于在安全RL后拒绝回答某些查询。如果没有安全RL，DeepSeek-R1的准确率可以达到70%以\n\n\n### 未来DeepSeek方向\n通用能力：目前，DeepSeek-R1在函数调用、多轮对话、复杂角色扮演和JSON输出等任务上的能力仍不及DeepSeek-V3。未来，我们计划探索如何利用长链推理(CoT)来增强这些领域的任务能力。\n语言混合：DeepSeek-R1目前优化了中文和英文，因此在处理其他语言的查询时可能会出现语言混用的问题。例如，DeepSeek-R1可能会使用英文进行推理和回答，即使查询本身是其他语言。我们计划在未来的更新中解决这一局限性。\n提示工程：在评估DeepSeek-R1时，我们观察到它对提示非常敏感。少量示例提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零-shot设置指定输出格式，以获得最佳结果。\n软件工程任务：由于评估时间较长，影响了强化学习过程的效率，大规模强化学习尚未在软件工程任务中广泛应用。因此，DeepSeek-R1在软件工程基准测试上未能相较于DeepSeek-V3表现出显著提升。未来版本将通过在软件工程数据上实施拒绝采样或在强化学习过程中引入异步评估来提高效率。\n\n\n","slug":"DeepSeek发展历程","published":1,"updated":"2025-03-01T21:59:37.185Z","comments":1,"layout":"post","photos":[],"_id":"cm7qqw7zw0000nvgsedkkg4fs","content":"<h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p>DeepSeek 依托其独特的架构，以极低丹成本卷赢了OpenAI等公司花费巨大成本<br>打造的AI模型，各大依托大语言模型提供服务科技公司纷纷将自己的后端替换为更廉价的DeepSeek，<br>它有什么样的魅力，它的出现又带来怎样翻天覆地的变化。</p>\n<p><img src=\"/personal-blog/DeepSeek%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/p1_header.png\" alt=\"p1_header.png\"></p>\n<h3 id=\"昂贵的AI模型\"><a href=\"#昂贵的AI模型\" class=\"headerlink\" title=\"昂贵的AI模型\"></a>昂贵的AI模型</h3><p>人类的大脑，内部有有许许多多的神经元，依靠大脑，人们可以对事物做出判断，付诸行动。<br>AI模型就像一颗人工大脑，它通过大量的样本学习，上百亿的参数参与计算，将习得经验<br>存储在硬盘中；然而，快速计算需要有超强算力的GPU、文件存储需要高IO，大缓存的存储后台 、<br>海量的人工标记样本也是一笔不菲的费用。</p>\n<p>现今，各家大厂都推出了各自的AI模型，其中OpenAI公司的OpenAI o1在数学，代码和推理任务<br>的效果最为优秀，然而，OpenAI技术是闭源的，并不对外公开，大众能看到的是其训练的所需巨额显卡算力。<br>然而又有多少家公司能像OpenAI一样投入这般财力和人力搞模型呢？</p>\n<p>答案是很少，英伟达作为显卡行业的佼佼者， 其显卡在AI训练的支持上最为先进，在行业里已经形成垄断<br>，价格也是不断上涨。但这并不能阻止投资者的疯狂，一个公司要训练一个智能的模型，购入大量价格高昂的显卡，<br>是不可避免的花销，对于普通人，若想要使用AI功能，只能通过网络API调用大厂模型开放的接口，在其基础上二次开发。</p>\n<p><img src=\"/personal-blog/DeepSeek%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/p1_money_cost.png\" alt=\"money_cost.png\"></p>\n<h3 id=\"百花齐放\"><a href=\"#百花齐放\" class=\"headerlink\" title=\"百花齐放\"></a>百花齐放</h3><p>过去几年，基于仅解码器 Transformer（Vaswani 等人，2017 年）的大型语言模型（LLM）<br>已逐渐成为实现通用人工智能 (AGI) 的基石和途径。 这一波浪潮是由闭源产品引领，例如 ChatGPT（OpenAI，2022）、<br>Claude（Anthropic，2023）和 Bard（Google，2023），这些产品利用了大量的计算资源和大量标注成本进行开发。<br>其中，LLaMA 系列模型创建了一个高效稳定的架构，构建了从 7B 到 70B 参数的性能优异模型<br>跟随 LLaMA，开源社区主要集中在训练固定大小（7B、13B、34B 和 70B）的高质量模型上，常常忽略对大型语言模型 (LLM) 规模规律的研究探索</p>\n<h3 id=\"DeepSeek-V1-LLMs——2万亿个大型数据集训练的开源大语言模型\"><a href=\"#DeepSeek-V1-LLMs——2万亿个大型数据集训练的开源大语言模型\" class=\"headerlink\" title=\"DeepSeek V1 LLMs——2万亿个大型数据集训练的开源大语言模型\"></a>DeepSeek V1 LLMs——2万亿个大型数据集训练的开源大语言模型</h3><p>DeepSeek的大语言模型，与现有的大语言模型在宏观设计方面略有不同。具体来说，DeepSeek LLM 7B 是一个 30 层网络，<br>而 DeepSeek LLM 67B 有 95 层。这些层级调整，在保持与其他开源模型参数一致性的同时，<br>也有利于模型流水线划分，优化训练和推理。而外的，他们采取分阶段微调的方式，第一阶段涉及使用所有可用数据进行微调，<br>而第二阶段专门关注使用对话数据进行微调，解决小型模型需要在数学和代码数据集上进行更长时间的微调后，模型对话能力被损坏的问题</p>\n<h3 id=\"DeepSeek-MOE-——-创新架构，经济高效\"><a href=\"#DeepSeek-MOE-——-创新架构，经济高效\" class=\"headerlink\" title=\"DeepSeek MOE —— 创新架构，经济高效\"></a>DeepSeek MOE —— 创新架构，经济高效</h3><p>在大型语言模型时代，DeepSeek团队引入龙混合专家（Mixture-of-Experts, MoE）架构，<br>对比先前的大数据模型，是一种有前途的架构，它的特点如下：</p>\n<p>细粒度专家分割：<br>在保持参数数量不变的同时，我们通过分割FFN中间隐藏维度将专家分割成更细粒度。<br>相应地，在保持恒定计算成本的情况下，我们还激活了更细粒度的专家，以实现更灵活、<br>适应性更强的激活专家组合。细粒度的专家细分允许将不同的知识更精细地分解，<br>并更精确地学习到不同的专家中，每个专家都将保持更高的专业水平。<br>此外，组合活跃专家的灵活性增加也有助于更准确、更有针对性地获取知识。</p>\n<p>共享专家隔离：<br>我们隔离某些专家，作为始终处于活动状态的共享专家，旨在在不同的环境中捕获和巩固共同知识。<br>通过将共同知识压缩到这些共享专家中，将减少其他路由专家之间的冗余。<br>这可以提高参数效率，并确保每个路由专家通过专注于独特的方面来保持专业性。<br>DeepSeekMoE中的这些架构创新为训练一个参数高效的MoE语言模型提供了机会，在这个模型中，<br>每个专家都是高度专业化的。</p>\n<p>DeepSeekMoE扩展到16B总参数的更大规模，在2T token上训练DeepSeekMoE 16B，<br>并展示了其与DeepSeek 7B和LLaMA2 7B相当的卓越性能，仅使用了约40%的计算量。<br>将DeepSeek MoE扩展到145B参数。DeepSeekMoE 145B仍然比GShard架构具有实质性的优势，<br>并且与DeepSeek 67B的性能相当，只使用了28.5%（甚至18.2%）的计算量。</p>\n<h3 id=\"DeepSeek-V2-——-强大、经济且高效的混合专家语言模型\"><a href=\"#DeepSeek-V2-——-强大、经济且高效的混合专家语言模型\" class=\"headerlink\" title=\"DeepSeek V2 —— 强大、经济且高效的混合专家语言模型\"></a>DeepSeek V2 —— 强大、经济且高效的混合专家语言模型</h3><p>DeepSeek-V2，一个支持 128K 上下文长度的大型 MoE 语言模型。 除了强大的性能之外，<br>它还具有经济训练和高效推理的特点，这得益于其包括 MLA 和 DeepSeekMoE 在内的创新架构。</p>\n<p>MLA：<br>除此之外，DeepSeek-V2设计了一种创新的注意力机制，称为多头潜在注意力（MLA）。<br>配备低秩键值联合压缩，MLA的性能优于MHA，但需要的KV缓存量要少得多。<br>它利用低秩键值联合压缩来消除推理时间键值缓存的瓶颈，从而支持高效的推理。</p>\n<p>DeepSeek-V2 一个支持 128K 上下文长度的大型 MoE 语言模型，仍处于Transformer架构中（Vaswani等人，2017），<br>只是其中每个Transformer 块由一个注意力模块和一个前馈网络（FFN）组成。</p>\n<p>与 DeepSeek 67B 相比，DeepSeek-V2 实现了显著增强的性能，同时节省了 42.5% 的训练成本、减少了 93.3% 的 KV 缓存、并将最大生成吞吐量提升至 5.76 倍。在由 8.1T 标记组成的高质量多源语料库上对 DeepSeek-V2 进行预训练，并进一步执行有监督微调 (SFT) 和强化学习 (RL) 以充分发挥其潜力。<br>评估结果表明，即使只有 21B 激活参数，DeepSeek-V2 及其聊天版本仍然在开源模型中实现了顶级性能。</p>\n<h3 id=\"DeepSeek-R1-强化学习激发潜能\"><a href=\"#DeepSeek-R1-强化学习激发潜能\" class=\"headerlink\" title=\"DeepSeek R1 强化学习激发潜能\"></a>DeepSeek R1 强化学习激发潜能</h3><p>先前的Deepseek版本在很大程度上依赖大量的监督数据来提高模型性能。<br>在DeepSeek团队研究中，各代产品展示的推理能力都可以通过大规模强化学习（RL）显著提升，<br>即使不使用监督微调（SFT）作为冷启动。此外，性能还可以通过加入少量冷启动数据进一步增强。</p>\n<p>产品列表：<br>（1）DeepSeek-R1-Zero，它直接将强化学习应用于基础模型，无需任何 SFT 数据；<br>（2）DeepSeek-R1，它从经过数千条长链式推理（CoT）示例微调的检查点开始应用强化学习；<br>（3）从 DeepSeek-R1 蒸馏推理能力到小型稠密模型。</p>\n<p>比较：<br>DeepSeek-R1在FRAMES方面表现出色，FRAMES是一项依赖于上下文的长期QA任务，<br>展示了其强大的文档分析能力。这突显了推理模型在人工智能驱动的搜索和数据分析任务中的潜力。<br>在事实基准SimpleQA上，DeepSeek-R1的表现优于DeepSeek-V3，展示了其处理基于事实的查询的能力。<br>在OpenAI-o1超过GPT-4o的情况下，也观察到了类似的趋势。<br>然而，在中国SimpleQA基准测试中，DeepSeek-R1的表现不如DeepSeek-V3，<br>主要是因为它倾向于在安全RL后拒绝回答某些查询。如果没有安全RL，DeepSeek-R1的准确率可以达到70%以</p>\n<h3 id=\"未来DeepSeek方向\"><a href=\"#未来DeepSeek方向\" class=\"headerlink\" title=\"未来DeepSeek方向\"></a>未来DeepSeek方向</h3><p>通用能力：目前，DeepSeek-R1在函数调用、多轮对话、复杂角色扮演和JSON输出等任务上的能力仍不及DeepSeek-V3。未来，我们计划探索如何利用长链推理(CoT)来增强这些领域的任务能力。<br>语言混合：DeepSeek-R1目前优化了中文和英文，因此在处理其他语言的查询时可能会出现语言混用的问题。例如，DeepSeek-R1可能会使用英文进行推理和回答，即使查询本身是其他语言。我们计划在未来的更新中解决这一局限性。<br>提示工程：在评估DeepSeek-R1时，我们观察到它对提示非常敏感。少量示例提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零-shot设置指定输出格式，以获得最佳结果。<br>软件工程任务：由于评估时间较长，影响了强化学习过程的效率，大规模强化学习尚未在软件工程任务中广泛应用。因此，DeepSeek-R1在软件工程基准测试上未能相较于DeepSeek-V3表现出显著提升。未来版本将通过在软件工程数据上实施拒绝采样或在强化学习过程中引入异步评估来提高效率。</p>\n","excerpt":"","more":"<h3 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h3><p>DeepSeek 依托其独特的架构，以极低丹成本卷赢了OpenAI等公司花费巨大成本<br>打造的AI模型，各大依托大语言模型提供服务科技公司纷纷将自己的后端替换为更廉价的DeepSeek，<br>它有什么样的魅力，它的出现又带来怎样翻天覆地的变化。</p>\n<p><img src=\"/personal-blog/DeepSeek%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/p1_header.png\" alt=\"p1_header.png\"></p>\n<h3 id=\"昂贵的AI模型\"><a href=\"#昂贵的AI模型\" class=\"headerlink\" title=\"昂贵的AI模型\"></a>昂贵的AI模型</h3><p>人类的大脑，内部有有许许多多的神经元，依靠大脑，人们可以对事物做出判断，付诸行动。<br>AI模型就像一颗人工大脑，它通过大量的样本学习，上百亿的参数参与计算，将习得经验<br>存储在硬盘中；然而，快速计算需要有超强算力的GPU、文件存储需要高IO，大缓存的存储后台 、<br>海量的人工标记样本也是一笔不菲的费用。</p>\n<p>现今，各家大厂都推出了各自的AI模型，其中OpenAI公司的OpenAI o1在数学，代码和推理任务<br>的效果最为优秀，然而，OpenAI技术是闭源的，并不对外公开，大众能看到的是其训练的所需巨额显卡算力。<br>然而又有多少家公司能像OpenAI一样投入这般财力和人力搞模型呢？</p>\n<p>答案是很少，英伟达作为显卡行业的佼佼者， 其显卡在AI训练的支持上最为先进，在行业里已经形成垄断<br>，价格也是不断上涨。但这并不能阻止投资者的疯狂，一个公司要训练一个智能的模型，购入大量价格高昂的显卡，<br>是不可避免的花销，对于普通人，若想要使用AI功能，只能通过网络API调用大厂模型开放的接口，在其基础上二次开发。</p>\n<p><img src=\"/personal-blog/DeepSeek%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B/p1_money_cost.png\" alt=\"money_cost.png\"></p>\n<h3 id=\"百花齐放\"><a href=\"#百花齐放\" class=\"headerlink\" title=\"百花齐放\"></a>百花齐放</h3><p>过去几年，基于仅解码器 Transformer（Vaswani 等人，2017 年）的大型语言模型（LLM）<br>已逐渐成为实现通用人工智能 (AGI) 的基石和途径。 这一波浪潮是由闭源产品引领，例如 ChatGPT（OpenAI，2022）、<br>Claude（Anthropic，2023）和 Bard（Google，2023），这些产品利用了大量的计算资源和大量标注成本进行开发。<br>其中，LLaMA 系列模型创建了一个高效稳定的架构，构建了从 7B 到 70B 参数的性能优异模型<br>跟随 LLaMA，开源社区主要集中在训练固定大小（7B、13B、34B 和 70B）的高质量模型上，常常忽略对大型语言模型 (LLM) 规模规律的研究探索</p>\n<h3 id=\"DeepSeek-V1-LLMs——2万亿个大型数据集训练的开源大语言模型\"><a href=\"#DeepSeek-V1-LLMs——2万亿个大型数据集训练的开源大语言模型\" class=\"headerlink\" title=\"DeepSeek V1 LLMs——2万亿个大型数据集训练的开源大语言模型\"></a>DeepSeek V1 LLMs——2万亿个大型数据集训练的开源大语言模型</h3><p>DeepSeek的大语言模型，与现有的大语言模型在宏观设计方面略有不同。具体来说，DeepSeek LLM 7B 是一个 30 层网络，<br>而 DeepSeek LLM 67B 有 95 层。这些层级调整，在保持与其他开源模型参数一致性的同时，<br>也有利于模型流水线划分，优化训练和推理。而外的，他们采取分阶段微调的方式，第一阶段涉及使用所有可用数据进行微调，<br>而第二阶段专门关注使用对话数据进行微调，解决小型模型需要在数学和代码数据集上进行更长时间的微调后，模型对话能力被损坏的问题</p>\n<h3 id=\"DeepSeek-MOE-——-创新架构，经济高效\"><a href=\"#DeepSeek-MOE-——-创新架构，经济高效\" class=\"headerlink\" title=\"DeepSeek MOE —— 创新架构，经济高效\"></a>DeepSeek MOE —— 创新架构，经济高效</h3><p>在大型语言模型时代，DeepSeek团队引入龙混合专家（Mixture-of-Experts, MoE）架构，<br>对比先前的大数据模型，是一种有前途的架构，它的特点如下：</p>\n<p>细粒度专家分割：<br>在保持参数数量不变的同时，我们通过分割FFN中间隐藏维度将专家分割成更细粒度。<br>相应地，在保持恒定计算成本的情况下，我们还激活了更细粒度的专家，以实现更灵活、<br>适应性更强的激活专家组合。细粒度的专家细分允许将不同的知识更精细地分解，<br>并更精确地学习到不同的专家中，每个专家都将保持更高的专业水平。<br>此外，组合活跃专家的灵活性增加也有助于更准确、更有针对性地获取知识。</p>\n<p>共享专家隔离：<br>我们隔离某些专家，作为始终处于活动状态的共享专家，旨在在不同的环境中捕获和巩固共同知识。<br>通过将共同知识压缩到这些共享专家中，将减少其他路由专家之间的冗余。<br>这可以提高参数效率，并确保每个路由专家通过专注于独特的方面来保持专业性。<br>DeepSeekMoE中的这些架构创新为训练一个参数高效的MoE语言模型提供了机会，在这个模型中，<br>每个专家都是高度专业化的。</p>\n<p>DeepSeekMoE扩展到16B总参数的更大规模，在2T token上训练DeepSeekMoE 16B，<br>并展示了其与DeepSeek 7B和LLaMA2 7B相当的卓越性能，仅使用了约40%的计算量。<br>将DeepSeek MoE扩展到145B参数。DeepSeekMoE 145B仍然比GShard架构具有实质性的优势，<br>并且与DeepSeek 67B的性能相当，只使用了28.5%（甚至18.2%）的计算量。</p>\n<h3 id=\"DeepSeek-V2-——-强大、经济且高效的混合专家语言模型\"><a href=\"#DeepSeek-V2-——-强大、经济且高效的混合专家语言模型\" class=\"headerlink\" title=\"DeepSeek V2 —— 强大、经济且高效的混合专家语言模型\"></a>DeepSeek V2 —— 强大、经济且高效的混合专家语言模型</h3><p>DeepSeek-V2，一个支持 128K 上下文长度的大型 MoE 语言模型。 除了强大的性能之外，<br>它还具有经济训练和高效推理的特点，这得益于其包括 MLA 和 DeepSeekMoE 在内的创新架构。</p>\n<p>MLA：<br>除此之外，DeepSeek-V2设计了一种创新的注意力机制，称为多头潜在注意力（MLA）。<br>配备低秩键值联合压缩，MLA的性能优于MHA，但需要的KV缓存量要少得多。<br>它利用低秩键值联合压缩来消除推理时间键值缓存的瓶颈，从而支持高效的推理。</p>\n<p>DeepSeek-V2 一个支持 128K 上下文长度的大型 MoE 语言模型，仍处于Transformer架构中（Vaswani等人，2017），<br>只是其中每个Transformer 块由一个注意力模块和一个前馈网络（FFN）组成。</p>\n<p>与 DeepSeek 67B 相比，DeepSeek-V2 实现了显著增强的性能，同时节省了 42.5% 的训练成本、减少了 93.3% 的 KV 缓存、并将最大生成吞吐量提升至 5.76 倍。在由 8.1T 标记组成的高质量多源语料库上对 DeepSeek-V2 进行预训练，并进一步执行有监督微调 (SFT) 和强化学习 (RL) 以充分发挥其潜力。<br>评估结果表明，即使只有 21B 激活参数，DeepSeek-V2 及其聊天版本仍然在开源模型中实现了顶级性能。</p>\n<h3 id=\"DeepSeek-R1-强化学习激发潜能\"><a href=\"#DeepSeek-R1-强化学习激发潜能\" class=\"headerlink\" title=\"DeepSeek R1 强化学习激发潜能\"></a>DeepSeek R1 强化学习激发潜能</h3><p>先前的Deepseek版本在很大程度上依赖大量的监督数据来提高模型性能。<br>在DeepSeek团队研究中，各代产品展示的推理能力都可以通过大规模强化学习（RL）显著提升，<br>即使不使用监督微调（SFT）作为冷启动。此外，性能还可以通过加入少量冷启动数据进一步增强。</p>\n<p>产品列表：<br>（1）DeepSeek-R1-Zero，它直接将强化学习应用于基础模型，无需任何 SFT 数据；<br>（2）DeepSeek-R1，它从经过数千条长链式推理（CoT）示例微调的检查点开始应用强化学习；<br>（3）从 DeepSeek-R1 蒸馏推理能力到小型稠密模型。</p>\n<p>比较：<br>DeepSeek-R1在FRAMES方面表现出色，FRAMES是一项依赖于上下文的长期QA任务，<br>展示了其强大的文档分析能力。这突显了推理模型在人工智能驱动的搜索和数据分析任务中的潜力。<br>在事实基准SimpleQA上，DeepSeek-R1的表现优于DeepSeek-V3，展示了其处理基于事实的查询的能力。<br>在OpenAI-o1超过GPT-4o的情况下，也观察到了类似的趋势。<br>然而，在中国SimpleQA基准测试中，DeepSeek-R1的表现不如DeepSeek-V3，<br>主要是因为它倾向于在安全RL后拒绝回答某些查询。如果没有安全RL，DeepSeek-R1的准确率可以达到70%以</p>\n<h3 id=\"未来DeepSeek方向\"><a href=\"#未来DeepSeek方向\" class=\"headerlink\" title=\"未来DeepSeek方向\"></a>未来DeepSeek方向</h3><p>通用能力：目前，DeepSeek-R1在函数调用、多轮对话、复杂角色扮演和JSON输出等任务上的能力仍不及DeepSeek-V3。未来，我们计划探索如何利用长链推理(CoT)来增强这些领域的任务能力。<br>语言混合：DeepSeek-R1目前优化了中文和英文，因此在处理其他语言的查询时可能会出现语言混用的问题。例如，DeepSeek-R1可能会使用英文进行推理和回答，即使查询本身是其他语言。我们计划在未来的更新中解决这一局限性。<br>提示工程：在评估DeepSeek-R1时，我们观察到它对提示非常敏感。少量示例提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零-shot设置指定输出格式，以获得最佳结果。<br>软件工程任务：由于评估时间较长，影响了强化学习过程的效率，大规模强化学习尚未在软件工程任务中广泛应用。因此，DeepSeek-R1在软件工程基准测试上未能相较于DeepSeek-V3表现出显著提升。未来版本将通过在软件工程数据上实施拒绝采样或在强化学习过程中引入异步评估来提高效率。</p>\n"}],"PostAsset":[{"_id":"source/_posts/DeepSeek发展历程/p1_header.png","slug":"p1_header.png","post":"cm7qqw7zw0000nvgsedkkg4fs","modified":0,"renderable":0},{"_id":"source/_posts/DeepSeek发展历程/p1_money_cost.png","slug":"p1_money_cost.png","post":"cm7qqw7zw0000nvgsedkkg4fs","modified":0,"renderable":0},{"_id":"source/_posts/DeepSeek发展历程/p1_r1_compare_other.png","slug":"p1_r1_compare_other.png","post":"cm7qqw7zw0000nvgsedkkg4fs","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[],"Tag":[]}}